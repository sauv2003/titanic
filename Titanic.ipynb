{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9920783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HADOOP_HOME'] = r'C:\\hadoop'\n",
    "os.environ['PATH'] += r';C:\\hadoop\\bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51f1fb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|NULL|    0|    0|          330877| 8.4583| NULL|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| NULL|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| NULL|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| NULL|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| NULL|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| NULL|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| NULL|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| NULL|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| NULL|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|NULL|    0|    0|          244373|   13.0| NULL|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| NULL|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|NULL|    0|    0|            2649|  7.225| NULL|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Starting the Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Titanic').getOrCreate()\n",
    "\n",
    "# Reading the data\n",
    "df = spark.read.csv('Titanic.csv',inferSchema=True, header=True)\n",
    "\n",
    "# Showing the data\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a974a06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82427e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "|Survived|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|\n",
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|       S|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|       C|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|       S|\n",
      "|       1|     1|female|35.0|    1|    0|   53.1|       S|\n",
      "|       0|     3|  male|35.0|    0|    0|   8.05|       S|\n",
      "|       0|     1|  male|54.0|    0|    0|51.8625|       S|\n",
      "|       0|     3|  male| 2.0|    3|    1| 21.075|       S|\n",
      "|       1|     3|female|27.0|    0|    2|11.1333|       S|\n",
      "|       1|     2|female|14.0|    1|    0|30.0708|       C|\n",
      "|       1|     3|female| 4.0|    1|    1|   16.7|       S|\n",
      "|       1|     1|female|58.0|    0|    0|  26.55|       S|\n",
      "|       0|     3|  male|20.0|    0|    0|   8.05|       S|\n",
      "|       0|     3|  male|39.0|    1|    5| 31.275|       S|\n",
      "|       0|     3|female|14.0|    0|    0| 7.8542|       S|\n",
      "|       1|     2|female|55.0|    0|    0|   16.0|       S|\n",
      "|       0|     3|  male| 2.0|    4|    1| 29.125|       Q|\n",
      "|       0|     3|female|31.0|    1|    0|   18.0|       S|\n",
      "|       0|     2|  male|35.0|    0|    0|   26.0|       S|\n",
      "|       1|     2|  male|34.0|    0|    0|   13.0|       S|\n",
      "|       1|     3|female|15.0|    0|    0| 8.0292|       Q|\n",
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting the columns which are required \n",
    "# to train and test the model.\n",
    "rm_columns = df.select(['Survived','Pclass',\n",
    "                       'Sex','Age','SibSp',\n",
    "                       'Parch','Fare','Embarked'])\n",
    "\n",
    "# Drops the data having null values\n",
    "result = rm_columns.na.drop()\n",
    "\n",
    "# Again showing the data\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f130a4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data  = result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c8bef4",
   "metadata": {},
   "source": [
    "### Convert String Columns to Ordinal Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc14a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "\n",
    "# Converting the Sex Column\n",
    "sexIdx = StringIndexer(inputCol='Sex',\n",
    "                               outputCol='SexIndex')\n",
    "sexEncode = OneHotEncoder(inputCol='SexIndex',\n",
    "                               outputCol='SexVec')\n",
    "\n",
    "# Converting the Embarked Column\n",
    "embarkIdx = StringIndexer(inputCol='Embarked',\n",
    "                               outputCol='EmbarkIndex')\n",
    "embarkEncode = OneHotEncoder(inputCol='EmbarkIndex',\n",
    "                               outputCol='EmbarkVec')\n",
    "\n",
    "# Vectorizing the data into a new column \"features\" \n",
    "# which will be our input/features class\n",
    "assembler = VectorAssembler(inputCols=['Pclass',\n",
    "                                       'SexVec','Age',\n",
    "                                       'SibSp','Parch',\n",
    "                                       'Fare','EmbarkVec'],\n",
    "                                    outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecfae829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Pipeline and Model\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(featuresCol='features',\n",
    "                             labelCol='Survived')\n",
    "\n",
    "# Creating the pipeline\n",
    "pipe = Pipeline(stages=[sexIdx, embarkIdx,\n",
    "                            sexEncode, embarkEncode,\n",
    "                            assembler, log_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eabb6900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+--------+--------+--------+-----------+-------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|Survived|Pclass|   Sex| Age|SibSp|Parch|    Fare|Embarked|SexIndex|EmbarkIndex|       SexVec|    EmbarkVec|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+------+------+----+-----+-----+--------+--------+--------+-----------+-------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|       0|     1|female|50.0|    0|    0| 28.7125|       C|     1.0|        1.0|    (1,[],[])|(2,[1],[1.0])|(8,[0,2,5,7],[1.0...|[-2.7623545463851...|[0.05939269222948...|       1.0|\n",
      "|       0|     1|  male|22.0|    0|    0|135.6333|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,22.0,0.0...|[-1.2697359816638...|[0.21930245102316...|       1.0|\n",
      "|       0|     1|  male|24.0|    0|    0|    79.2|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,24.0,0.0...|[-1.2390726299604...|[0.22459744915948...|       1.0|\n",
      "|       0|     1|  male|28.0|    1|    0| 82.1708|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,28.0,1.0...|[-0.7251349432791...|[0.32626323722067...|       1.0|\n",
      "|       0|     1|  male|29.0|    1|    0|    66.6|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,29.0,1.0...|[0.23600164197516...|[0.55872808278835...|       0.0|\n",
      "|       0|     1|  male|36.0|    1|    0|   78.85|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,36.0,1.0...|[0.53941676873140...|[0.63167673285126...|       0.0|\n",
      "|       0|     1|  male|38.0|    0|    0|     0.0|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|(8,[0,1,2,6],[1.0...|[0.20480042186945...|[0.55102189482616...|       0.0|\n",
      "|       0|     1|  male|39.0|    0|    0|     0.0|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|(8,[0,1,2,6],[1.0...|[0.24650950973675...|[0.56131718664506...|       0.0|\n",
      "|       0|     1|  male|45.0|    0|    0|    35.5|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,45.0,0.0...|[0.52995005039546...|[0.62947146193980...|       0.0|\n",
      "|       0|     1|  male|46.0|    0|    0|    79.2|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,46.0,0.0...|[-0.3214726968798...|[0.42031688232406...|       1.0|\n",
      "|       0|     1|  male|47.0|    0|    0| 25.5875|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,47.0,0.0...|[0.60410184983789...|[0.64659418316619...|       0.0|\n",
      "|       0|     1|  male|49.0|    1|    1|110.8833|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,49.0,1.0...|[0.19782824619433...|[0.54929639428177...|       0.0|\n",
      "|       0|     1|  male|51.0|    0|    1| 61.3792|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,51.0,0.0...|[-0.1093550462381...|[0.47268845017426...|       1.0|\n",
      "|       0|     1|  male|56.0|    0|    0| 30.6958|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,56.0,0.0...|[0.05027561746015...|[0.51256625756433...|       0.0|\n",
      "|       0|     1|  male|58.0|    0|    0|    29.7|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,58.0,0.0...|[0.13276290214691...|[0.53314205970192...|       0.0|\n",
      "|       0|     1|  male|61.0|    0|    0|    33.5|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,61.0,0.0...|[1.19542582171138...|[0.76771006332680...|       0.0|\n",
      "|       0|     1|  male|62.0|    0|    0|   26.55|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,62.0,0.0...|[1.23063792947976...|[0.77393020734504...|       0.0|\n",
      "|       0|     1|  male|62.0|    0|    0|   26.55|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,62.0,0.0...|[1.23063792947976...|[0.77393020734504...|       0.0|\n",
      "|       0|     1|  male|65.0|    0|    1| 61.9792|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,65.0,0.0...|[0.47513307427229...|[0.61659796442140...|       0.0|\n",
      "|       0|     1|  male|70.0|    1|    1|    71.0|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,70.0,1.0...|[1.97041884366439...|[0.87765609399612...|       0.0|\n",
      "+--------+------+------+----+-----+-----+--------+--------+--------+-----------+-------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into train and test\n",
    "train_data, test_data = final_data.randomSplit([0.7, .3])\n",
    "\n",
    "# Fitting the model on training data\n",
    "fit_model = pipe.fit(train_data)\n",
    "\n",
    "# Storing the results on test data\n",
    "results = fit_model.transform(test_data)\n",
    "\n",
    "# Showing the results\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ceec6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the evaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Calling the evaluator\n",
    "res = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='Survived')\n",
    "\n",
    "# Evaluating the AUC on results\n",
    "ROC_AUC = res.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a31dc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7772551433093019\n"
     ]
    }
   ],
   "source": [
    "print(ROC_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47992911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "categorical = ['Sex','Embarked']\n",
    "numeric = ['Pclass','Age','SibSp','Parch','Fare']\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical),\n",
    "        ('num', 'passthrough', numeric)\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = Pipeline([\n",
    "    ('pre', pre),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d814412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8037383177570093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Saurabh\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv(\"Titanic.csv\")\n",
    "\n",
    "# Keep required columns\n",
    "df = df[['Survived','Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']]\n",
    "\n",
    "# Drop rows with NA\n",
    "df = df.dropna()\n",
    "\n",
    "# Split features and labels\n",
    "X = df.drop(\"Survived\", axis=1)\n",
    "y = df[\"Survived\"]\n",
    "\n",
    "categorical = ['Sex','Embarked']\n",
    "numeric = ['Pclass','Age','SibSp','Parch','Fare']\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical),\n",
    "        ('num', 'passthrough', numeric)\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = Pipeline([\n",
    "    ('pre', pre),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Fit Model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy\n",
    "score = model.score(X_test, y_test)\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d07b0f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['titanic_model.pkl']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, \"titanic_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "50aabb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"titanic_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea557759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "sample = pd.DataFrame([{\n",
    "    \"Pclass\": 1,\n",
    "    \"Sex\": \"female\",\n",
    "    \"Age\": 50,\n",
    "    \"SibSp\": 8,\n",
    "    \"Parch\": 0,\n",
    "    \"Fare\": 27.25,\n",
    "    \"Embarked\": \"C\"\n",
    "}])\n",
    "\n",
    "print(model.predict(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "05198c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('pre',\n",
      "                 ColumnTransformer(transformers=[('cat',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  ['Sex', 'Embarked']),\n",
      "                                                 ('num', 'passthrough',\n",
      "                                                  ['Pclass', 'Age', 'SibSp',\n",
      "                                                   'Parch', 'Fare'])])),\n",
      "                ('clf', LogisticRegression())])\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3762ac79",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1773898957.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[54], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install scikit-learn==1.4.2\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn==1.4.2\n",
    "pip install numpy==1.26.4\n",
    "pip install pandas==2.2.2\n",
    "pip install joblib\n",
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b0d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
